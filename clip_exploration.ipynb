{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Check: Cal Poly Survivor： S3 E10： Loved Ones [Kggc-m8ntVQ].mp4\n",
      "Caption: \"previously on C paully\"\n",
      "Timestamp: 00:00:00,359 → 00:00:02,350\n",
      "Frame Indices: [10, 25, 40, 55, 70]\n",
      "Frames tensor shape: torch.Size([5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import re\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load CLIP model and preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define directories\n",
    "data_dir = \"data\"\n",
    "videos_dir = os.path.join(data_dir, \"videos\")\n",
    "captions_dir = os.path.join(data_dir, \"captions\")\n",
    "\n",
    "# Function to extract YouTube video ID from filename\n",
    "def extract_video_id(filename):\n",
    "    match = re.search(r\"\\[([A-Za-z0-9_-]+)\\]\", filename)  # Extracts text inside brackets [videoID]\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Get first available video file\n",
    "video_files = [f for f in os.listdir(videos_dir) if f.endswith(\".mp4\")]\n",
    "if not video_files:\n",
    "    print(\"No video files found!\")\n",
    "    exit()\n",
    "\n",
    "video_file = video_files[0]  # Grab the first video\n",
    "video_path = os.path.join(videos_dir, video_file)\n",
    "\n",
    "# Find corresponding caption file\n",
    "video_id = extract_video_id(video_file)\n",
    "if not video_id:\n",
    "    print(f\"Could not extract video ID from {video_file}\")\n",
    "    exit()\n",
    "\n",
    "caption_file = next((f for f in os.listdir(captions_dir) if video_id in f and f.endswith(\".json\")), None)\n",
    "if not caption_file:\n",
    "    print(f\"No matching caption file found for {video_file}\")\n",
    "    exit()\n",
    "\n",
    "caption_path = os.path.join(captions_dir, caption_file)\n",
    "\n",
    "# Load the caption JSON\n",
    "with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Grab the first timestamp and its frames\n",
    "if not captions:\n",
    "    print(\"Caption file is empty!\")\n",
    "    exit()\n",
    "\n",
    "first_entry = captions[0]\n",
    "start_time = first_entry[\"start_time\"]\n",
    "end_time = first_entry[\"end_time\"]\n",
    "caption_text = first_entry[\"caption\"]\n",
    "frame_indices = first_entry.get(\"frames\", [])\n",
    "\n",
    "if not frame_indices:\n",
    "    print(\"No frame indices found in the first caption entry!\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nSanity Check: {video_file}\")\n",
    "print(f\"Caption: \\\"{caption_text}\\\"\")\n",
    "print(f\"Timestamp: {start_time} → {end_time}\")\n",
    "print(f\"Frame Indices: {frame_indices}\")\n",
    "\n",
    "# Extract frames and convert them into tensors\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_tensors = []\n",
    "\n",
    "for frame_idx in frame_indices:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Convert BGR (OpenCV) to RGB (PIL)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Apply CLIP preprocessing\n",
    "        frame_tensor = preprocess(pil_image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        frame_tensors.append(frame_tensor)\n",
    "    else:\n",
    "        print(f\"❌ Failed to retrieve frame {frame_idx}\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Stack frames into a single tensor (batch)\n",
    "if frame_tensors:\n",
    "    frames_tensor = torch.cat(frame_tensors, dim=0)\n",
    "    print(f\"Frames tensor shape: {frames_tensor.shape}\")  # (batch_size, 3, 224, 224)\n",
    "else:\n",
    "    print(\"❌ No valid frames were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: -1.6611\n",
      "Epoch 2/2 - Loss: -2.1896\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)  # Small LR to fine-tune\n",
    "\n",
    "num_epochs = 2\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for frame in frames_tensor:\n",
    "        frame = frame.unsqueeze(0).to(device)\n",
    "        caption = clip.tokenize([caption_text]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Encode image and text\n",
    "        image_features = model.encode_image(frame)\n",
    "        text_features = model.encode_text(caption)\n",
    "\n",
    "        # Normalize\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "\n",
    "        # Contrastive loss (maximize similarity)\n",
    "        loss = -F.cosine_similarity(image_features, text_features).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: -0.6126\n",
      "Epoch 2/10 - Loss: -0.6434\n",
      "Epoch 3/10 - Loss: -0.6847\n",
      "Epoch 4/10 - Loss: -0.7088\n",
      "Epoch 5/10 - Loss: -0.7417\n",
      "Epoch 6/10 - Loss: -0.7683\n",
      "Epoch 7/10 - Loss: -0.7877\n",
      "Epoch 8/10 - Loss: -0.8100\n",
      "Epoch 9/10 - Loss: -0.8290\n",
      "Epoch 10/10 - Loss: -0.8451\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)  # Small LR to fine-tune\n",
    "\n",
    "num_epochs = 10\n",
    "frames_tensor = frames_tensor.to(device)  # Move all frames to device once\n",
    "captions_tokenized = clip.tokenize([caption_text] * frames_tensor.shape[0]).to(device)  # Repeat caption\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Encode all images and text in a single batch\n",
    "    image_features = model.encode_image(frames_tensor)\n",
    "    text_features = model.encode_text(captions_tokenized)\n",
    "\n",
    "    # Normalize\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "    # Contrastive loss (maximize similarity)\n",
    "    #TODO add contrastive loss as this was mentioned in the github for this model\n",
    "    loss = -F.cosine_similarity(image_features, text_features).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
