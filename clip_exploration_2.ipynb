{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Check: Cal Poly Survivor： S3 E8： Like a Mob Boss [jJePD7jcNBQ].mp4\n",
      "Caption: \"previously on C paully\"\n",
      "Timestamp: 00:00:00,359 → 00:00:02,350\n",
      "Frame Indices: [10, 25, 40, 55, 70]\n",
      "Frames tensor shape: torch.Size([5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import re\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load CLIP model and preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define directories\n",
    "data_dir = \"data\"\n",
    "videos_dir = os.path.join(data_dir, \"videos\")\n",
    "captions_dir = os.path.join(data_dir, \"captions\")\n",
    "\n",
    "# Function to extract YouTube video ID from filename\n",
    "def extract_video_id(filename):\n",
    "    match = re.search(r\"\\[([A-Za-z0-9_-]+)\\]\", filename)  # Extracts text inside brackets [videoID]\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Get first available video file\n",
    "video_files = [f for f in os.listdir(videos_dir) if f.endswith(\".mp4\")]\n",
    "if not video_files:\n",
    "    print(\"No video files found!\")\n",
    "    exit()\n",
    "\n",
    "video_file = video_files[0]  # Grab the first video\n",
    "video_path = os.path.join(videos_dir, video_file)\n",
    "\n",
    "# Find corresponding caption file\n",
    "video_id = extract_video_id(video_file)\n",
    "if not video_id:\n",
    "    print(f\"Could not extract video ID from {video_file}\")\n",
    "    exit()\n",
    "\n",
    "caption_file = next((f for f in os.listdir(captions_dir) if video_id in f and f.endswith(\".json\")), None)\n",
    "if not caption_file:\n",
    "    print(f\"No matching caption file found for {video_file}\")\n",
    "    exit()\n",
    "\n",
    "caption_path = os.path.join(captions_dir, caption_file)\n",
    "\n",
    "# Load the caption JSON\n",
    "with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Grab the first timestamp and its frames\n",
    "if not captions:\n",
    "    print(\"Caption file is empty!\")\n",
    "    exit()\n",
    "\n",
    "first_entry = captions[0]\n",
    "start_time = first_entry[\"start_time\"]\n",
    "end_time = first_entry[\"end_time\"]\n",
    "caption_text = first_entry[\"caption\"]\n",
    "frame_indices = first_entry.get(\"frames\", [])\n",
    "\n",
    "if not frame_indices:\n",
    "    print(\"No frame indices found in the first caption entry!\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nSanity Check: {video_file}\")\n",
    "print(f\"Caption: \\\"{caption_text}\\\"\")\n",
    "print(f\"Timestamp: {start_time} → {end_time}\")\n",
    "print(f\"Frame Indices: {frame_indices}\")\n",
    "\n",
    "# Extract frames and convert them into tensors\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_tensors = []\n",
    "\n",
    "for frame_idx in frame_indices:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Convert BGR (OpenCV) to RGB (PIL)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Apply CLIP preprocessing\n",
    "        frame_tensor = preprocess(pil_image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        frame_tensors.append(frame_tensor)\n",
    "    else:\n",
    "        print(f\"❌ Failed to retrieve frame {frame_idx}\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Stack frames into a single tensor (batch)\n",
    "if frame_tensors:\n",
    "    frames_tensor = torch.cat(frame_tensors, dim=0)\n",
    "    print(f\"Frames tensor shape: {frames_tensor.shape}\")  # (batch_size, 3, 224, 224)\n",
    "else:\n",
    "    print(\"❌ No valid frames were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class LazyFrameCaptionDataset(Dataset):\n",
    "    def __init__(self, videos_dir, captions_dir, tokenizer, preprocess, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocess = preprocess\n",
    "        self.metadata = []\n",
    "\n",
    "        video_files = [f for f in os.listdir(videos_dir) if f.endswith(\".mp4\")]\n",
    "        for video_file in video_files:\n",
    "            video_path = os.path.join(videos_dir, video_file)\n",
    "            video_id = extract_video_id(video_file)\n",
    "\n",
    "            if not video_id:\n",
    "                continue\n",
    "\n",
    "            # Match caption file\n",
    "            caption_file = next((f for f in os.listdir(captions_dir) if video_id in f and f.endswith(\".json\")), None)\n",
    "            if not caption_file:\n",
    "                continue\n",
    "\n",
    "            caption_path = os.path.join(captions_dir, caption_file)\n",
    "            with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                captions = json.load(f)\n",
    "\n",
    "            for entry in captions:\n",
    "                if not entry.get(\"frames\"):\n",
    "                    continue\n",
    "\n",
    "                self.metadata.append({\n",
    "                    \"video_path\": video_path,\n",
    "                    \"frame_indices\": entry[\"frames\"],\n",
    "                    \"caption\": entry[\"caption\"]\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.metadata[idx]\n",
    "        frame_indices = entry[\"frame_indices\"]\n",
    "        caption_text = entry[\"caption\"]\n",
    "        video_path = entry[\"video_path\"]\n",
    "\n",
    "        # Lazy-load one random frame\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_idx = random.choice(frame_indices)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if not ret:\n",
    "            frame_tensor = torch.zeros((3, 224, 224), dtype=torch.float)\n",
    "        else:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "            frame_tensor = self.preprocess(pil_image).to(self.device)\n",
    "\n",
    "        text_tokens = self.tokenizer([caption_text], truncate=True).squeeze(0).to(self.device)\n",
    "\n",
    "        return frame_tensor, text_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = LazyFrameCaptionDataset(videos_dir, captions_dir, clip.tokenize, preprocess, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def clip_contrastive_loss(image_features, text_features, temperature=0.07):\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "    logits_per_image = image_features @ text_features.T\n",
    "    logits_per_text = text_features @ image_features.T\n",
    "\n",
    "    logits_per_image /= temperature\n",
    "    logits_per_text /= temperature\n",
    "\n",
    "    targets = torch.arange(image_features.shape[0], device=image_features.device)\n",
    "    loss_i2t = F.cross_entropy(logits_per_image, targets)\n",
    "    loss_t2i = F.cross_entropy(logits_per_text, targets)\n",
    "\n",
    "    return (loss_i2t + loss_t2i) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Loss: 3.2319\n",
      "Epoch 2/10 - Avg Loss: 2.7050\n",
      "Epoch 3/10 - Avg Loss: 2.2439\n",
      "Epoch 4/10 - Avg Loss: 1.8727\n",
      "Epoch 5/10 - Avg Loss: 1.5761\n",
      "Epoch 6/10 - Avg Loss: 1.3329\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      7\u001b[39m     total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CSC487/final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CSC487/final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CSC487/final-project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mLazyFrameCaptionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     48\u001b[39m cap = cv2.VideoCapture(video_path)\n\u001b[32m     49\u001b[39m frame_idx = random.choice(frame_indices)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m ret, frame = cap.read()\n\u001b[32m     52\u001b[39m cap.release()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# Create directory to save model checkpoints\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, text_tokens in dataloader:\n",
    "        images = images.to(device)\n",
    "        text_tokens = text_tokens.to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "        loss = clip_contrastive_loss(image_features, text_features)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(save_dir, f\"clip_epoch_{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    print(f\"✅ Model saved to {checkpoint_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
